{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a02501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from data_loader import HotpotQALoader\n",
    "from rag_pipeline import VectorStore, LLMGenerator, RAGPipeline\n",
    "from evaluator import Evaluator\n",
    "from utils import set_random_seed, load_config\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9be62",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config/config.yaml')\n",
    "set_random_seed(config['seed'])\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Subset size: {config['dataset']['subset_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493608e",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared subset\n",
    "loader = HotpotQALoader(subset_size=config['dataset']['subset_size'])\n",
    "loader.load_subset('../data/hotpotqa_subset.json')\n",
    "\n",
    "print(f\"Loaded {len(loader.subset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c73c8",
   "metadata": {},
   "source": [
    "## 3. Build Retrieval Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "import json\n",
    "with open('../data/corpus.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "print(f\"Loaded corpus with {len(corpus)} passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector store\n",
    "vector_store = VectorStore(encoder_model=config['retrieval']['encoder_model'])\n",
    "vector_store.build_index(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1bf4da",
   "metadata": {},
   "source": [
    "## 4. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8647ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval on a sample question\n",
    "test_question = loader.subset[0]['question']\n",
    "print(f\"Question: {test_question}\")\n",
    "\n",
    "retrieved, scores = vector_store.retrieve(test_question, k=3)\n",
    "\n",
    "print(f\"\\nTop 3 retrieved passages:\")\n",
    "for i, (passage, score) in enumerate(zip(retrieved, scores), 1):\n",
    "    print(f\"\\n{i}. [Score: {score:.3f}] {passage['title']}\")\n",
    "    print(f\"   {passage['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98205f7c",
   "metadata": {},
   "source": [
    "## 5. Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load language model\n",
    "# WARNING: This may take several minutes and requires significant memory\n",
    "\n",
    "generator = LLMGenerator(\n",
    "    model_name=config['model']['name'],\n",
    "    device=config['model']['device'],\n",
    "    load_in_8bit=config['model']['load_in_8bit']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ce982",
   "metadata": {},
   "source": [
    "## 6. Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b091186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "rag_pipeline = RAGPipeline(vector_store, generator)\n",
    "\n",
    "print(\"RAG pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d8ac7",
   "metadata": {},
   "source": [
    "## 7. Test Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on one example\n",
    "test_example = loader.preprocess_example(loader.subset[0])\n",
    "question = test_example['question']\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground truth: {test_example['answer']}\")\n",
    "\n",
    "# No RAG\n",
    "result_no_rag = rag_pipeline.answer_without_rag(\n",
    "    question,\n",
    "    temperature=config['model']['temperature'],\n",
    "    max_new_tokens=config['model']['max_new_tokens']\n",
    ")\n",
    "print(f\"\\nNo-RAG answer: {result_no_rag['answer']}\")\n",
    "\n",
    "# With RAG (k=3)\n",
    "result_rag = rag_pipeline.answer_with_rag(\n",
    "    question,\n",
    "    k=3,\n",
    "    temperature=config['model']['temperature'],\n",
    "    max_new_tokens=config['model']['max_new_tokens']\n",
    ")\n",
    "print(f\"\\nRAG k=3 answer: {result_rag['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08d503",
   "metadata": {},
   "source": [
    "## 8. Run Full Experiment\n",
    "\n",
    "**Note:** This will take significant time. Consider running on a subset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06913565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on small subset for testing (adjust as needed)\n",
    "TEST_SIZE = 10  # Set to len(loader.subset) for full run\n",
    "\n",
    "evaluator = Evaluator()\n",
    "\n",
    "generation_kwargs = {\n",
    "    'temperature': config['model']['temperature'],\n",
    "    'max_new_tokens': config['model']['max_new_tokens'],\n",
    "    'seed': config['seed']\n",
    "}\n",
    "\n",
    "for i in tqdm(range(TEST_SIZE), desc=\"Processing examples\"):\n",
    "    example = loader.preprocess_example(loader.subset[i])\n",
    "    question = example['question']\n",
    "    \n",
    "    # No-RAG baseline\n",
    "    no_rag_result = rag_pipeline.answer_without_rag(question, **generation_kwargs)\n",
    "    evaluator.evaluate_single(example, no_rag_result, condition='no_rag')\n",
    "    \n",
    "    # RAG with different k values\n",
    "    for k in config['retrieval']['k_values']:\n",
    "        rag_result = rag_pipeline.answer_with_rag(question, k=k, **generation_kwargs)\n",
    "        evaluator.evaluate_single(example, rag_result, condition=f'rag_k{k}')\n",
    "\n",
    "print(\"\\nExperiment completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b1411",
   "metadata": {},
   "source": [
    "## 9. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results dataframe\n",
    "results_df = evaluator.get_results_df()\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by condition\n",
    "for condition in ['no_rag', 'rag_k1', 'rag_k3', 'rag_k5']:\n",
    "    metrics = evaluator.aggregate_results(condition=condition)\n",
    "    print(f\"\\n{condition}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf87696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "comparison = evaluator.compare_conditions('no_rag', 'rag_k3')\n",
    "\n",
    "print(\"\\nNo-RAG vs RAG k=3:\")\n",
    "print(f\"EM difference: {comparison['em_diff']:.3f}\")\n",
    "print(f\"F1 difference: {comparison['f1_diff']:.3f}\")\n",
    "print(f\"Hallucination reduction: {comparison['hallucination_diff']:.3f}\")\n",
    "print(f\"McNemar p-value: {comparison['mcnemar_p_value']:.4f}\")\n",
    "print(f\"Significant: {comparison['mcnemar_p_value'] < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46058d06",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10884f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "evaluator.save_results('../results/evaluation_results.csv')\n",
    "\n",
    "print(\"Results saved to results/evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c5468",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Run full experiment (set TEST_SIZE = len(loader.subset))\n",
    "- Visualize results in notebook 03\n",
    "- Perform error analysis\n",
    "- Manual hallucination annotation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
